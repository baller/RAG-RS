#!/usr/bin/env python3
"""
æµ‹è¯•ä¸åŒbatch sizeä¸‹çš„å­¦ä¹ ç‡ç¼©æ”¾ç­–ç•¥
"""

from src.models.embedding import MultiModalEmbeddingModel
import torch

def test_lr_scaling():
    """æµ‹è¯•å­¦ä¹ ç‡ç¼©æ”¾åŠŸèƒ½"""
    
    print("ğŸ“Š æ‰¹æ¬¡å¤§å°ä¸å­¦ä¹ ç‡ç¼©æ”¾æµ‹è¯•")
    print("=" * 60)
    
    # åŸºç¡€é…ç½®
    base_config = {
        'embed_dim': 512,
        'temperature': 0.07,
        'learning_rate': 1e-4,  # åŸºç¡€å­¦ä¹ ç‡
        'backbone': 'vit_b_16',
        'log_wandb': False
    }
    
    # å­¦ä¹ ç‡ç¼©æ”¾é…ç½®
    lr_scaling_config = {
        'enabled': True,
        'rule': 'linear',
        'base_batch_size': 32
    }
    
    # æµ‹è¯•ä¸åŒçš„batch size
    batch_sizes = [16, 32, 64, 128, 320, 640, 1024]
    
    print("\n1. çº¿æ€§ç¼©æ”¾ç­–ç•¥ (Linear Scaling)")
    print("-" * 40)
    for batch_size in batch_sizes:
        model = MultiModalEmbeddingModel(
            **base_config,
            lr_scaling=lr_scaling_config,
            batch_size=batch_size
        )
        scale_factor = batch_size / 32
        print(f"  Batch Size: {batch_size:4d} | LR: {model.learning_rate:.2e} | Scale: {scale_factor:.2f}x")
    
    print("\n2. å¹³æ–¹æ ¹ç¼©æ”¾ç­–ç•¥ (Square Root Scaling)")
    print("-" * 40)
    sqrt_config = lr_scaling_config.copy()
    sqrt_config['rule'] = 'sqrt'
    
    for batch_size in batch_sizes:
        model = MultiModalEmbeddingModel(
            **base_config,
            lr_scaling=sqrt_config,
            batch_size=batch_size
        )
        scale_factor = (batch_size / 32) ** 0.5
        print(f"  Batch Size: {batch_size:4d} | LR: {model.learning_rate:.2e} | Scale: {scale_factor:.2f}x")
    
    print("\n3. ä¸ç¼©æ”¾ç­–ç•¥ (No Scaling)")
    print("-" * 40)
    no_scaling_config = lr_scaling_config.copy()
    no_scaling_config['rule'] = 'none'
    
    for batch_size in batch_sizes[:3]:  # åªæµ‹è¯•å‡ ä¸ª
        model = MultiModalEmbeddingModel(
            **base_config,
            lr_scaling=no_scaling_config,
            batch_size=batch_size
        )
        print(f"  Batch Size: {batch_size:4d} | LR: {model.learning_rate:.2e} | Scale: 1.00x")

def test_temperature_adjustment():
    """æµ‹è¯•æ¸©åº¦å‚æ•°è°ƒæ•´"""
    
    print("\nğŸŒ¡ï¸ æ¸©åº¦å‚æ•°è°ƒæ•´æµ‹è¯•")
    print("=" * 60)
    
    base_config = {
        'embed_dim': 512,
        'temperature': 0.07,
        'learning_rate': 1e-4,
        'backbone': 'vit_b_16',
        'log_wandb': False
    }
    
    batch_sizes = [16, 32, 64, 128, 320, 640, 1024]
    
    for batch_size in batch_sizes:
        model = MultiModalEmbeddingModel(**base_config, batch_size=batch_size)
        adjusted_temp = model._adjust_temperature_for_batch_size(0.07, batch_size)
        ratio = adjusted_temp / 0.07
        print(f"  Batch Size: {batch_size:4d} | Temperature: {adjusted_temp:.4f} | Ratio: {ratio:.3f}x")

def demonstrate_contrastive_scaling():
    """æ¼”ç¤ºå¯¹æ¯”å­¦ä¹ ä¸­batch sizeçš„å½±å“"""
    
    print("\nğŸ¯ å¯¹æ¯”å­¦ä¹ batch sizeå½±å“æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡å‹
    model = MultiModalEmbeddingModel(
        embed_dim=256, 
        temperature=0.07, 
        log_wandb=False
    )
    model.eval()
    
    batch_sizes = [8, 32, 128]
    
    for batch_size in batch_sizes:
        # åˆ›å»ºéšæœºembedding
        emb1 = torch.randn(batch_size, 256)
        emb2 = torch.randn(batch_size, 256)
        
        # å½’ä¸€åŒ–
        emb1 = torch.nn.functional.normalize(emb1, p=2, dim=1)
        emb2 = torch.nn.functional.normalize(emb2, p=2, dim=1)
        
        # è®¡ç®—å¯¹æ¯”æŸå¤±
        loss = model.contrastive_loss(emb1, emb2)
        
        # è®¡ç®—è´Ÿæ ·æœ¬æ•°é‡
        negative_samples = batch_size - 1
        
        print(f"  Batch Size: {batch_size:3d} | è´Ÿæ ·æœ¬æ•°: {negative_samples:3d} | Loss: {loss.item():.4f}")

def recommendations():
    """è¾“å‡ºå»ºè®®"""
    
    print("\nğŸ’¡ å®ç”¨å»ºè®®")
    print("=" * 60)
    
    recommendations = [
        "1. çº¿æ€§ç¼©æ”¾ (Linear Scaling):",
        "   - é€‚ç”¨äºå¤§å¤šæ•°æƒ…å†µ",
        "   - batch_sizeå¢å¤§kå€ â†’ lrå¢å¤§kå€",
        "   - å»ºè®®: batch_size=320æ—¶, lr=1e-3",
        "",
        "2. å¹³æ–¹æ ¹ç¼©æ”¾ (Square Root Scaling):",
        "   - æ›´ä¿å®ˆçš„ç¼©æ”¾ç­–ç•¥",
        "   - é€‚ç”¨äºå¯¹å­¦ä¹ ç‡æ•æ„Ÿçš„æ¨¡å‹",
        "   - batch_sizeå¢å¤§kå€ â†’ lrå¢å¤§âˆškå€",
        "",
        "3. Warmupç­–ç•¥:",
        "   - å¤§batch sizeè®­ç»ƒå»ºè®®ä½¿ç”¨warmup",
        "   - å»ºè®®warmup_epochs = 10-20",
        "   - ä»0æˆ–å°å­¦ä¹ ç‡é€æ¸å¢åŠ åˆ°ç›®æ ‡å­¦ä¹ ç‡",
        "",
        "4. æ¸©åº¦å‚æ•°è°ƒæ•´:",
        "   - å¤§batch sizeå¯èƒ½éœ€è¦ç¨å¾®å¢åŠ æ¸©åº¦",
        "   - å¸®åŠ©å¹³è¡¡æ›´å¤šè´Ÿæ ·æœ¬å¸¦æ¥çš„å­¦ä¹ éš¾åº¦",
        "",
        "5. å¯¹æ¯”å­¦ä¹ ç‰¹æ®Šè€ƒè™‘:",
        "   - æ›´å¤§batch size = æ›´å¤šè´Ÿæ ·æœ¬ = æ›´éš¾çš„å¯¹æ¯”ä»»åŠ¡",
        "   - å¯èƒ½éœ€è¦è°ƒæ•´æŸå¤±æƒé‡æˆ–æ¸©åº¦å‚æ•°",
        "   - å»ºè®®ä»å°batch sizeå¼€å§‹è°ƒè¯•"
    ]
    
    for rec in recommendations:
        print(f"  {rec}")

if __name__ == "__main__":
    test_lr_scaling()
    test_temperature_adjustment()
    demonstrate_contrastive_scaling()
    recommendations()
    
    print(f"\nâœ… å­¦ä¹ ç‡ç¼©æ”¾æµ‹è¯•å®Œæˆï¼") 