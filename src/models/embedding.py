import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import lightning as L
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models import vit_b_16, ViT_B_16_Weights
import math
import wandb


class AerialEncoder(nn.Module):
    """Aerialç¼–ç å™¨ï¼Œä¸“é—¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæ•°æ®ï¼Œæ”¯æŒViTå’ŒResNet backbone"""
    def __init__(self, input_channels=4, embed_dim=512, backbone='vit_b_16'):
        super().__init__()
        self.input_channels = input_channels
        self.embed_dim = embed_dim
        self.backbone_type = backbone
        
        if backbone == 'resnet50':
            # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet50ä½œä¸ºbackbone
            backbone_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)
            
            # ä¿®æ”¹ç¬¬ä¸€å±‚å·ç§¯ä»¥é€‚åº”ä¸åŒçš„è¾“å…¥é€šé“æ•°
            if input_channels != 3:
                backbone_model.conv1 = nn.Conv2d(
                    input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False
                )
            
            # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
            backbone_model.fc = nn.Identity()
            self.backbone = backbone_model
            backbone_output_dim = 2048
            
        elif backbone == 'vit_b_16':
            # ä½¿ç”¨é¢„è®­ç»ƒçš„Vision Transformer
            backbone_model = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)
            
            # ä¿®æ”¹patch embeddingä»¥é€‚åº”ä¸åŒçš„è¾“å…¥é€šé“æ•°
            if input_channels != 3:
                original_conv = backbone_model.conv_proj
                backbone_model.conv_proj = nn.Conv2d(
                    input_channels, 
                    original_conv.out_channels,
                    kernel_size=original_conv.kernel_size,
                    stride=original_conv.stride,
                    padding=original_conv.padding,
                    bias=original_conv.bias is not None
                )
                
                # å¦‚æœè¾“å…¥é€šé“æ•°ä¸æ˜¯3ï¼Œéœ€è¦è°ƒæ•´æƒé‡
                if input_channels != 3:
                    with torch.no_grad():
                        if input_channels < 3:
                            # å¦‚æœè¾“å…¥é€šé“å°‘äº3ï¼Œå–å‰å‡ ä¸ªé€šé“çš„æƒé‡
                            backbone_model.conv_proj.weight.data = original_conv.weight.data[:, :input_channels, :, :]
                        else:
                            # å¦‚æœè¾“å…¥é€šé“å¤šäº3ï¼Œé‡å¤RGBæƒé‡
                            new_weight = original_conv.weight.data.repeat(1, input_channels // 3 + 1, 1, 1)
                            backbone_model.conv_proj.weight.data = new_weight[:, :input_channels, :, :]
            
            # ç§»é™¤åˆ†ç±»å¤´
            backbone_model.heads = nn.Identity()
            self.backbone = backbone_model
            backbone_output_dim = 768  # ViT-B/16çš„è¾“å‡ºç»´åº¦
            
        else:
            raise ValueError(f"AerialEncoderä¸æ”¯æŒçš„backboneç±»å‹: {backbone}")
        
        # æŠ•å½±å¤´ï¼šå°†backboneè¾“å‡ºæ˜ å°„åˆ°embeddingç©ºé—´
        self.projection_head = nn.Sequential(
            nn.Linear(backbone_output_dim, embed_dim * 2),
            nn.BatchNorm1d(embed_dim * 2),
            nn.ReLU(inplace=True),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.BatchNorm1d(embed_dim)
        )
        
    def forward(self, x):
        # é€šè¿‡backboneæå–ç‰¹å¾
        features = self.backbone(x)  # (B, backbone_output_dim)
        
        # é€šè¿‡æŠ•å½±å¤´å¾—åˆ°embedding
        embeddings = self.projection_head(features)  # (B, embed_dim)
        
        # L2å½’ä¸€åŒ–
        embeddings = F.normalize(embeddings, p=2, dim=1)
        
        return embeddings


class MLPEncoder(nn.Module):
    """MLPç¼–ç å™¨ï¼Œä¸“é—¨å¤„ç†å°å°ºå¯¸æ—¶åºæ•°æ®ï¼ˆS1, S2ï¼‰ï¼Œä½¿ç”¨meanæ—¶åºèšåˆ"""
    def __init__(self, input_channels, embed_dim=512, spatial_size=6, dropout=0.1):
        super().__init__()
        self.input_channels = input_channels
        self.embed_dim = embed_dim
        self.spatial_size = spatial_size
        
        # è®¡ç®—å±•å¹³åçš„ç‰¹å¾ç»´åº¦
        # è¾“å…¥: (B, T, C, H, W) -> meanèšåˆå: (B, C, H, W) -> å±•å¹³: (B, C*H*W)
        input_dim = input_channels * spatial_size * spatial_size
        
        # MLPæŠ•å½±ç½‘ç»œ
        self.projection = nn.Sequential(
            # ç¬¬ä¸€å±‚ï¼šè¾“å…¥ç»´åº¦åˆ°ä¸­é—´ç»´åº¦
            nn.Linear(input_dim, embed_dim * 4),
            nn.BatchNorm1d(embed_dim * 4),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            
            # ç¬¬äºŒå±‚ï¼šä¸­é—´ç»´åº¦åˆ°ä¸­é—´ç»´åº¦  
            nn.Linear(embed_dim * 4, embed_dim * 2),
            nn.BatchNorm1d(embed_dim * 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            
            # ç¬¬ä¸‰å±‚ï¼šä¸­é—´ç»´åº¦åˆ°ç›®æ ‡ç»´åº¦
            nn.Linear(embed_dim * 2, embed_dim),
            nn.BatchNorm1d(embed_dim)
        )
    
    def forward(self, x):
        # å¤„ç†æ—¶åºæ•°æ®
        if x.dim() == 5:  # (B, T, C, H, W) - æ‰¹æ¬¡æ—¶åºæ•°æ®
            # ä½¿ç”¨meanè¿›è¡Œæ—¶åºèšåˆ
            x = torch.mean(x, dim=1)  # (B, C, H, W)
        elif x.dim() == 4 and x.shape[0] > 1:  # (T, C, H, W) - å•æ ·æœ¬æ—¶åºæ•°æ®
            # å¦‚æœæ˜¯å•æ ·æœ¬çš„æ—¶åºæ•°æ®ï¼Œä¹Ÿè¿›è¡Œmeanèšåˆ
            x = torch.mean(x, dim=0, keepdim=True)  # (1, C, H, W)
        
        # å±•å¹³ç©ºé—´ç»´åº¦
        batch_size = x.size(0)
        x = x.view(batch_size, -1)  # (B, C*H*W)
        
        # é€šè¿‡MLPå¾—åˆ°embedding
        embeddings = self.projection(x)  # (B, embed_dim)
        
        # L2å½’ä¸€åŒ–
        embeddings = F.normalize(embeddings, p=2, dim=1)
        
        return embeddings


class MultiModalEmbeddingModel(L.LightningModule):
    """å¤šæ¨¡æ€embeddingè®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨å¼‚æ„ç¼–ç å™¨æ¶æ„"""
    
    def __init__(
        self,
        embed_dim=1024,
        temperature=0.07,
        learning_rate=1e-4,
        weight_decay=1e-4,
        warmup_epochs=20,
        max_epochs=200,
        modality_weights=None,
        backbone='vit_b_16',
        log_wandb=True,
        lr_scaling=None,
        batch_size=None
    ):
        super().__init__()
        self.save_hyperparameters()
        
        # å¼‚æ„ç¼–ç å™¨æ¶æ„
        # Aerial: é«˜åˆ†è¾¨ç‡å›¾åƒ â†’ AerialEncoder(ViT/ResNet)
        self.aerial_encoder = AerialEncoder(
            input_channels=4, 
            embed_dim=embed_dim, 
            backbone=backbone
        )
        
        # S1: 2é€šé“Ã—6Ã—6æ—¶åº â†’ MLPEncoder(meanèšåˆ)
        self.s1_encoder = MLPEncoder(
            input_channels=2,
            embed_dim=embed_dim,
            spatial_size=6
        )
        
        # S2: 10é€šé“Ã—6Ã—6æ—¶åº â†’ MLPEncoder(meanèšåˆ)
        self.s2_encoder = MLPEncoder(
            input_channels=10,
            embed_dim=embed_dim,
            spatial_size=6
        )
        
        # è¶…å‚æ•°
        self.temperature = temperature
        self.base_learning_rate = learning_rate  # ä¿å­˜åŸå§‹å­¦ä¹ ç‡
        self.weight_decay = weight_decay
        self.warmup_epochs = warmup_epochs
        self.max_epochs = max_epochs
        self.backbone = backbone
        self.log_wandb = log_wandb
        
        # å­¦ä¹ ç‡ç¼©æ”¾é€»è¾‘
        self.learning_rate = self._calculate_scaled_lr(learning_rate, lr_scaling, batch_size)
        
        # æ¨¡æ€æƒé‡ (aerial-s1, aerial-s2, s1-s2)
        if modality_weights is None:
            self.modality_weights = [1.0, 1.0, 1.0]
        else:
            self.modality_weights = modality_weights
            
        # ç”¨äºç›‘æ§è®­ç»ƒè¿‡ç¨‹
        self.automatic_optimization = True
    
    def _calculate_scaled_lr(self, base_lr, lr_scaling, batch_size):
        """æ ¹æ®batch sizeç¼©æ”¾å­¦ä¹ ç‡"""
        if lr_scaling is None or not lr_scaling.get('enabled', False) or batch_size is None:
            return base_lr
        
        rule = lr_scaling.get('rule', 'linear')
        base_batch_size = lr_scaling.get('base_batch_size', 32)
        
        scale_factor = batch_size / base_batch_size
        
        if rule == 'linear':
            scaled_lr = base_lr * scale_factor
        elif rule == 'sqrt':
            scaled_lr = base_lr * math.sqrt(scale_factor)
        elif rule == 'none':
            scaled_lr = base_lr
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­¦ä¹ ç‡ç¼©æ”¾è§„åˆ™: {rule}")
        
        print(f"ğŸ”§ å­¦ä¹ ç‡ç¼©æ”¾: {base_lr:.2e} â†’ {scaled_lr:.2e} (batch_size: {batch_size}, rule: {rule})")
        return scaled_lr
    
    def _adjust_temperature_for_batch_size(self, base_temp, batch_size):
        """æ ¹æ®batch sizeè°ƒæ•´æ¸©åº¦å‚æ•°
        
        å¯¹æ¯”å­¦ä¹ ä¸­ï¼Œæ›´å¤§çš„batch sizeæ„å‘³ç€æ›´å¤šè´Ÿæ ·æœ¬ï¼Œ
        å¯èƒ½éœ€è¦ç¨å¾®è°ƒæ•´æ¸©åº¦å‚æ•°æ¥å¹³è¡¡å­¦ä¹ éš¾åº¦
        """
        if batch_size is None:
            return base_temp
            
        # ç»éªŒå…¬å¼ï¼šæ¸©åº¦éšbatch sizeå¯¹æ•°å¢é•¿è€Œè½»å¾®å¢åŠ 
        # è¿™æœ‰åŠ©äºåœ¨å¤§batch sizeæ—¶ä¿æŒåˆé€‚çš„å­¦ä¹ éš¾åº¦
        temp_adjustment = 1.0 + 0.01 * math.log(batch_size / 32.0) if batch_size > 32 else 1.0
        adjusted_temp = base_temp * temp_adjustment
        
        if abs(adjusted_temp - base_temp) > 0.001:
            print(f"ğŸŒ¡ï¸ æ¸©åº¦å‚æ•°è°ƒæ•´: {base_temp:.3f} â†’ {adjusted_temp:.3f} (batch_size: {batch_size})")
        
        return adjusted_temp
        
    def contrastive_loss(self, embeddings1, embeddings2):
        """è®¡ç®—ä¸¤ä¸ªæ¨¡æ€ä¹‹é—´çš„å¯¹æ¯”å­¦ä¹ æŸå¤±"""
        batch_size = embeddings1.size(0)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.mm(embeddings1, embeddings2.t()) / self.temperature
        
        # æ ‡ç­¾ï¼šå¯¹è§’çº¿ä¸Šçš„å…ƒç´ ä¸ºæ­£æ ·æœ¬
        labels = torch.arange(batch_size).to(self.device)
        
        # è®¡ç®—äº¤å‰ç†µæŸå¤±
        loss_12 = F.cross_entropy(similarity_matrix, labels)
        loss_21 = F.cross_entropy(similarity_matrix.t(), labels)
        
        return (loss_12 + loss_21) / 2
    
    def triple_contrastive_loss(self, aerial_emb, s1_emb, s2_emb):
        """è®¡ç®—ä¸‰æ¨¡æ€å¯¹æ¯”å­¦ä¹ æŸå¤±"""
        # è®¡ç®—ä¸‰ç§ä¸¤ä¸¤å¯¹æ¯”æŸå¤±
        loss_aerial_s1 = self.contrastive_loss(aerial_emb, s1_emb)
        loss_aerial_s2 = self.contrastive_loss(aerial_emb, s2_emb)
        loss_s1_s2 = self.contrastive_loss(s1_emb, s2_emb)
        
        # åŠ æƒç»„åˆ
        total_loss = (
            self.modality_weights[0] * loss_aerial_s1 +
            self.modality_weights[1] * loss_aerial_s2 +
            self.modality_weights[2] * loss_s1_s2
        ) / sum(self.modality_weights)
        
        # è®°å½•å„ä¸ªæŸå¤±åˆ†é‡
        loss_dict = {
            'aerial_s1_loss': loss_aerial_s1,
            'aerial_s2_loss': loss_aerial_s2,
            's1_s2_loss': loss_s1_s2,
            'total_loss': total_loss
        }
        
        return total_loss, loss_dict
    
    def forward(self, batch):
        """å‰å‘ä¼ æ’­"""
        embeddings = {}
        
        # æå–å„æ¨¡æ€çš„embedding
        if 'aerial' in batch:
            embeddings['aerial'] = self.aerial_encoder(batch['aerial'])
        
        if 's1' in batch:
            embeddings['s1'] = self.s1_encoder(batch['s1'])
        
        if 's2' in batch:
            embeddings['s2'] = self.s2_encoder(batch['s2'])
            
        return embeddings
    
    def training_step(self, batch, batch_idx):
        """è®­ç»ƒæ­¥éª¤"""
        # è·å–embeddings
        embeddings = self(batch)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ¨¡æ€
        available_modalities = list(embeddings.keys())
        if len(available_modalities) < 2:
            return None
            
        # è®¡ç®—æŸå¤±
        if len(available_modalities) == 3:
            # å¦‚æœä¸‰ä¸ªæ¨¡æ€éƒ½å­˜åœ¨
            total_loss, loss_dict = self.triple_contrastive_loss(
                embeddings['aerial'], embeddings['s1'], embeddings['s2']
            )
        else:
            # å¦‚æœåªæœ‰ä¸¤ä¸ªæ¨¡æ€ï¼Œè®¡ç®—å¯¹åº”çš„å¯¹æ¯”æŸå¤±
            modality_pairs = {
                ('aerial', 's1'): 0,
                ('aerial', 's2'): 1,
                ('s1', 's2'): 2
            }
            
            total_loss = 0
            loss_dict = {}
            count = 0
            
            for (mod1, mod2), weight_idx in modality_pairs.items():
                if mod1 in available_modalities and mod2 in available_modalities:
                    loss = self.contrastive_loss(embeddings[mod1], embeddings[mod2])
                    total_loss += self.modality_weights[weight_idx] * loss
                    loss_dict[f'{mod1}_{mod2}_loss'] = loss
                    count += 1
            
            if count > 0:
                total_loss /= count
                loss_dict['total_loss'] = total_loss
        
        # è®°å½•æŸå¤± - å­¦ä¹ OmniSatçš„æ—¥å¿—è®°å½•æ–¹å¼
        for key, value in loss_dict.items():
            self.log(
                f'train/{key}',
                value,
                on_step=True,
                on_epoch=True,
                prog_bar=True,
                sync_dist=True
            )
        
        # è®¡ç®—embeddingè´¨é‡æŒ‡æ ‡
        self._log_embedding_metrics(embeddings, 'train')
        
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤"""
        embeddings = self(batch)
        
        available_modalities = list(embeddings.keys())
        if len(available_modalities) < 2:
            return None
            
        # è®¡ç®—éªŒè¯æŸå¤±
        if len(available_modalities) == 3:
            total_loss, loss_dict = self.triple_contrastive_loss(
                embeddings['aerial'], embeddings['s1'], embeddings['s2']
            )
        else:
            # å¤„ç†ä¸¤ä¸ªæ¨¡æ€çš„æƒ…å†µ
            modality_pairs = [('aerial', 's1'), ('aerial', 's2'), ('s1', 's2')]
            total_loss = 0
            loss_dict = {}
            count = 0
            
            for mod1, mod2 in modality_pairs:
                if mod1 in available_modalities and mod2 in available_modalities:
                    loss = self.contrastive_loss(embeddings[mod1], embeddings[mod2])
                    total_loss += loss
                    loss_dict[f'{mod1}_{mod2}_loss'] = loss
                    count += 1
            
            if count > 0:
                total_loss /= count
                loss_dict['total_loss'] = total_loss
        
        # è®°å½•éªŒè¯æŸå¤± - å­¦ä¹ OmniSatçš„æ—¥å¿—è®°å½•æ–¹å¼
        for key, value in loss_dict.items():
            self.log(
                f'val/{key}',
                value,
                on_step=False,
                on_epoch=True,
                prog_bar=True,
                sync_dist=True
            )
        
        # è®¡ç®—embeddingè´¨é‡æŒ‡æ ‡
        self._log_embedding_metrics(embeddings, 'val')
        
        return total_loss
    
    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤"""
        embeddings = self(batch)
        
        available_modalities = list(embeddings.keys())
        if len(available_modalities) < 2:
            return None
            
        # è®¡ç®—æµ‹è¯•æŸå¤±
        if len(available_modalities) == 3:
            total_loss, loss_dict = self.triple_contrastive_loss(
                embeddings['aerial'], embeddings['s1'], embeddings['s2']
            )
        else:
            # å¤„ç†ä¸¤ä¸ªæ¨¡æ€çš„æƒ…å†µ
            modality_pairs = [('aerial', 's1'), ('aerial', 's2'), ('s1', 's2')]
            total_loss = 0
            loss_dict = {}
            count = 0
            
            for mod1, mod2 in modality_pairs:
                if mod1 in available_modalities and mod2 in available_modalities:
                    loss = self.contrastive_loss(embeddings[mod1], embeddings[mod2])
                    total_loss += loss
                    loss_dict[f'{mod1}_{mod2}_loss'] = loss
                    count += 1
            
            if count > 0:
                total_loss /= count
                loss_dict['total_loss'] = total_loss
        
        # è®°å½•æµ‹è¯•æŸå¤±
        for key, value in loss_dict.items():
            self.log(
                f'test/{key}',
                value,
                on_step=False,
                on_epoch=True,
                prog_bar=True,
                sync_dist=True
            )
        
        # è®¡ç®—embeddingè´¨é‡æŒ‡æ ‡
        self._log_embedding_metrics(embeddings, 'test')
        
        return total_loss
    
    def _log_embedding_metrics(self, embeddings, stage):
        """è®°å½•embeddingè´¨é‡æŒ‡æ ‡"""
        for modality, emb in embeddings.items():
            # embeddingæ ‡å‡†å·®ï¼ˆè¡¡é‡ç‰¹å¾åˆ†å¸ƒï¼‰
            std = emb.std().item()
            self.log(
                f'{stage}/{modality}_embedding_std',
                std,
                on_epoch=True,
                sync_dist=True
            )
            
            # embeddingèŒƒæ•°
            norm = emb.norm(dim=1).mean().item()
            self.log(
                f'{stage}/{modality}_embedding_norm',
                norm,
                on_epoch=True,
                sync_dist=True
            )
        
        # æ¨¡æ€é—´ç›¸ä¼¼åº¦
        if len(embeddings) >= 2:
            modalities = list(embeddings.keys())
            for i in range(len(modalities)):
                for j in range(i+1, len(modalities)):
                    mod1, mod2 = modalities[i], modalities[j]
                    # è®¡ç®—æ‰¹æ¬¡å†…å¹³å‡ç›¸ä¼¼åº¦
                    similarity = F.cosine_similarity(
                        embeddings[mod1].mean(0), embeddings[mod2].mean(0), dim=0
                    ).item()
                    self.log(
                        f'{stage}/{mod1}_{mod2}_similarity',
                        similarity,
                        on_epoch=True,
                        sync_dist=True
                    )
    
    def _log_wandb_metrics(self, embeddings, loss_dict, stage):
        """è®°å½•è¯¦ç»†çš„wandbæŒ‡æ ‡"""
        if not hasattr(wandb, 'log') or wandb.run is None:
            return
            
        # å½“å‰å­¦ä¹ ç‡
        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']
        
        # åŸºç¡€æŒ‡æ ‡
        wandb_log = {
            f'{stage}/learning_rate': current_lr,
            f'{stage}/epoch': self.current_epoch,
            f'{stage}/global_step': self.trainer.global_step,
        }
        
        # æŸå¤±å‡½æ•°æŒ‡æ ‡
        for key, value in loss_dict.items():
            wandb_log[f'{stage}/{key}'] = value.item() if torch.is_tensor(value) else value
        
        # Embeddingç»Ÿè®¡ä¿¡æ¯
        for modality, emb in embeddings.items():
            if emb.numel() > 0:
                # åŸºç¡€ç»Ÿè®¡
                wandb_log.update({
                    f'{stage}/{modality}_emb_mean': emb.mean().item(),
                    f'{stage}/{modality}_emb_std': emb.std().item(),
                    f'{stage}/{modality}_emb_min': emb.min().item(),
                    f'{stage}/{modality}_emb_max': emb.max().item(),
                    f'{stage}/{modality}_emb_norm_mean': emb.norm(dim=1).mean().item(),
                })
                
                # æ¢¯åº¦ç»Ÿè®¡ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
                if stage == 'train' and hasattr(self, f'{modality}_encoder'):
                    encoder = getattr(self, f'{modality}_encoder')
                    total_norm = 0
                    param_count = 0
                    for param in encoder.parameters():
                        if param.grad is not None:
                            param_norm = param.grad.data.norm(2)
                            total_norm += param_norm.item() ** 2
                            param_count += 1
                    if param_count > 0:
                        total_norm = total_norm ** (1. / 2)
                        wandb_log[f'{stage}/{modality}_grad_norm'] = total_norm
        
        # æ¨¡æ€é—´ç›¸ä¼¼åº¦çŸ©é˜µ
        if len(embeddings) >= 2:
            modalities = list(embeddings.keys())
            similarity_matrix = {}
            for i, mod1 in enumerate(modalities):
                for j, mod2 in enumerate(modalities):
                    if i != j and embeddings[mod1].numel() > 0 and embeddings[mod2].numel() > 0:
                        # è®¡ç®—æ‰¹æ¬¡å†…çš„ç›¸ä¼¼åº¦
                        sim = F.cosine_similarity(
                            embeddings[mod1].mean(0, keepdim=True), 
                            embeddings[mod2].mean(0, keepdim=True), 
                            dim=1
                        ).item()
                        similarity_matrix[f'{mod1}_{mod2}'] = sim
            
            # è®°å½•ç›¸ä¼¼åº¦çŸ©é˜µ
            if similarity_matrix:
                wandb_log[f'{stage}/similarity_matrix'] = similarity_matrix
        
        # æ¸©åº¦å‚æ•°
        wandb_log[f'{stage}/temperature'] = self.temperature
        
        # æ¨¡å‹æ¶æ„ä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€æ¬¡è®°å½•ï¼‰
        if self.trainer.global_step == 0 and stage == 'train':
            wandb_log.update({
                'model/backbone': self.backbone,
                'model/embed_dim': self.hparams.embed_dim,
                'model/total_params': sum(p.numel() for p in self.parameters()),
                'model/trainable_params': sum(p.numel() for p in self.parameters() if p.requires_grad),
            })
        
        # è®°å½•åˆ°wandb (è®©Lightningè‡ªåŠ¨ç®¡ç†æ­¥æ•°)
        wandb.log(wandb_log)
        
        # å¯è§†åŒ–embeddingåˆ†å¸ƒï¼ˆæ¯100æ­¥è®°å½•ä¸€æ¬¡ï¼‰
        if stage == 'val' and self.trainer.global_step % 100 == 0:
            self._log_embedding_distributions(embeddings)
    
    def _log_embedding_distributions(self, embeddings):
        """è®°å½•embeddingåˆ†å¸ƒçš„ç›´æ–¹å›¾"""
        if not hasattr(wandb, 'log') or wandb.run is None:
            return
            
        try:
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(1, len(embeddings), figsize=(5*len(embeddings), 4))
            if len(embeddings) == 1:
                axes = [axes]
            
            for idx, (modality, emb) in enumerate(embeddings.items()):
                if emb.numel() > 0:
                    # ç»˜åˆ¶embeddingåˆ†å¸ƒç›´æ–¹å›¾
                    emb_flat = emb.detach().cpu().numpy().flatten()
                    axes[idx].hist(emb_flat, bins=50, alpha=0.7, density=True)
                    axes[idx].set_title(f'{modality.upper()} Embedding Distribution')
                    axes[idx].set_xlabel('Value')
                    axes[idx].set_ylabel('Density')
                    axes[idx].grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # è®°å½•åˆ°wandb
            wandb.log({
                'embeddings/distribution': wandb.Image(fig)
            })
            
            plt.close(fig)
            
        except Exception as e:
            print(f"ç»˜åˆ¶embeddingåˆ†å¸ƒæ—¶å‡ºé”™: {e}")
    
    def configure_optimizers(self):
        """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.learning_rate,
            weight_decay=self.weight_decay
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼šwarmup + cosine annealing
        def lr_lambda(epoch):
            if epoch < self.warmup_epochs:
                return epoch / self.warmup_epochs
            else:
                return 0.5 * (1 + math.cos(math.pi * (epoch - self.warmup_epochs) / 
                                          (self.max_epochs - self.warmup_epochs)))
        
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
        
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'interval': 'epoch'
            }
        }


class EmbeddingExtractor:
    """ç”¨äºæå–embeddingçš„å·¥å…·ç±»"""
    
    def __init__(self, model_path, device='cuda'):
        self.device = device
        self.model = MultiModalEmbeddingModel.load_from_checkpoint(model_path)
        self.model.eval()
        self.model.to(device)
    
    def extract_embeddings(self, dataloader, modalities=['aerial', 's1', 's2']):
        """ä»æ•°æ®åŠ è½½å™¨ä¸­æå–embeddings"""
        all_embeddings = {mod: [] for mod in modalities}
        all_labels = []
        all_names = []
        
        with torch.no_grad():
            for batch in dataloader:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                for key in batch:
                    if torch.is_tensor(batch[key]):
                        batch[key] = batch[key].to(self.device)
                
                # æå–embeddings
                embeddings = self.model(batch)
                
                # æ”¶é›†ç»“æœ
                for mod in modalities:
                    if mod in embeddings:
                        all_embeddings[mod].append(embeddings[mod].cpu())
                
                all_labels.append(batch['label'].cpu())
                all_names.extend(batch['name'])
        
        # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ
        for mod in modalities:
            if all_embeddings[mod]:
                all_embeddings[mod] = torch.cat(all_embeddings[mod], dim=0)
        
        all_labels = torch.cat(all_labels, dim=0)
        
        return all_embeddings, all_labels, all_names
